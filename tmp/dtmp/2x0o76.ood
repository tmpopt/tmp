{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL6J0xtg3ams"
      },
      "outputs": [],
      "source": [
        "#IMDB_sentiment_Classification\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idlchnJq6zUF",
        "outputId": "8628648b-a8ab-493b-eaf9-2afc5aed4841"
      },
      "outputs": [],
      "source": [
        "#loading imdb data with most frequent 10000 words\n",
        "\n",
        "from keras.datasets import imdb\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
        "data = np.concatenate((X_train, X_test), axis=0)\n",
        "label = np.concatenate((y_train, y_test), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9fxt_He-8ML",
        "outputId": "3b3ec207-d487-419e-cfc3-06fbac2d6d30"
      },
      "outputs": [],
      "source": [
        "print(\"Review is \",X_train[5]) # series of no converted word to vocabulory associated with index\n",
        "print(\"Review is \",y_train[5]) # 0 indicating a negative review and 1 indicating a positive review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6laDpGAeJjJ7"
      },
      "outputs": [],
      "source": [
        "# Now we split our data into a training and a testing set. \n",
        "# The training set will contain  reviews and the testing set \n",
        "\n",
        "test_x = data[:10000]\n",
        "test_y = label[:10000]\n",
        "train_x = data[10000:]\n",
        "train_y = label[10000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMpdzIkW-848",
        "outputId": "a3950525-6d77-4fd6-9039-1308ec10d1b8"
      },
      "outputs": [],
      "source": [
        "print(\"Categories:\", np.unique(label))\n",
        "print(\"Number of unique words:\", len(np.unique(np.hstack(data))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq38Xbjy_jlv",
        "outputId": "5a0f5a5d-660f-4672-d31e-e66d6fef0485"
      },
      "outputs": [],
      "source": [
        "# Let's decode the first review\n",
        "# Above you see the first review of the dataset which is labeled as positive (1).\n",
        "# The code below retrieves the dictionary mapping word indices back into the original words so that we can read them. \n",
        "# It replaces every unknown word with a “#”. It does this by using the get_word_index() function.\n",
        "\n",
        "\n",
        "# Retrieves a dict mapping words to their index in the IMDB dataset.\n",
        "index = imdb.get_word_index()\n",
        "# If there is a possibility of multiple keys with the same value, you will need to specify the desired behaviour in this case to lookup more than 2 values\n",
        "# ivd = dict((v, k) for k, v in d.items())\n",
        "# If you want to peek at the reviews yourself and see what people have actually written, you can reverse the process too:\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
        "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in data[0]] ) #The purpose of subtracting 3 from i is to adjust the indice,# to indicate that the index was not found.\n",
        "print(decoded) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7ACaI6ECaU8"
      },
      "outputs": [],
      "source": [
        "#Adding sequence to data\n",
        "# Vectorization is the process of converting textual data into numerical vectors and is a process that is usually applied once the text is cleaned.\n",
        "data = vectorize(data)\n",
        "label = np.array(label).astype(\"float32\")\n",
        "\n",
        "# Now it is time to prepare our data. We will vectorize every review and fill it with zeros so that it contains exactly 1000 numbers. \n",
        "# That means we fill every review that is shorter than 1000 with zeros. \n",
        "# We do this because the biggest review is nearly that long and every input for our neural network needs to have the same size. \n",
        "# We also transform the targets into floats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoHZ4IPDDCjL"
      },
      "outputs": [],
      "source": [
        "# Let's check distribution of data\n",
        "\n",
        "# To create plots for EDA(exploratory data analysis) \n",
        "import seaborn as sns #seaborn is a popular Python visualization library that is built on top of Matplotlib and provides a high-level interface for creating informative and attractive statistical graphics.\n",
        "sns.set(color_codes=True)\n",
        "import matplotlib.pyplot as plt # %matplotlib to display Matplotlib plots inline with the notebook\n",
        "%matplotlib inline "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "WD3bOTy6DGO_",
        "outputId": "d8601ccb-0c08-4d3a-92b9-94ff63b12a07"
      },
      "outputs": [],
      "source": [
        "labelDF=pd.DataFrame({'label':label})\n",
        "sns.countplot(x='label', data=labelDF)\n",
        "\n",
        "# For below analysis it is clear that data has equel distribution of sentiments.This will help us building a good model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3VNm4V0DJ2h"
      },
      "outputs": [],
      "source": [
        "# Creating train and test data set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data,label, test_size=0.20, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85VbeXfBEgca"
      },
      "outputs": [],
      "source": [
        "# Let's create sequential model,In deep learning, a Sequential model is a linear stack of layers, where you can simply add layers one after the other\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras import models\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Phrv18kKEo2_",
        "outputId": "e1e472e2-8321-4c75-d3a5-28090112ccf5"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "# Input - Layer\n",
        "# Note that we set the input-shape to 10,000 at the input-layer because our reviews are 10,000 integers long.\n",
        "# The input-layer takes 10,000 as input and outputs it with a shape of 50.\n",
        "model.add(layers.Dense(50, activation = \"relu\", input_shape=(10000, )))\n",
        "# Hidden - Layers\n",
        "# Please note you should always use a dropout rate between 20% and 50%. # here in our case 0.3 means 30% dropout we are using dropout to prevent overfitting. \n",
        "# By the way, if you want you can build a sentiment analysis without LSTMs(Long Short-Term Memory networks), then you simply need to replace it by a flatten layer:\n",
        "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation = \"relu\")) #ReLU\" stands for Rectified Linear Unit, and it is a commonly used activation function in neural networks. \n",
        "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation = \"relu\"))\n",
        "# Output- Layer\n",
        "model.add(layers.Dense(1, activation = \"sigmoid\")) #adds another Dense layer to the model, but with a single neuron instead of 50,i.e. out put layer,it produces the output predictions of the model.\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXAKnIRaE7KH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf \n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKv2l42xHKuN"
      },
      "outputs": [],
      "source": [
        "# We use the “adam” optimizer, an algorithm that changes the weights and biases during training.\n",
        "# During training, the weights and biases of a machine learning model are updated iteratively to minimize the difference between the model's predictions and the actual outputs.\n",
        "# We also choose binary-crossentropy as loss (because we deal with binary classification) and accuracy as our evaluation metric.\n",
        "\n",
        "model.compile(\n",
        " optimizer = \"adam\",\n",
        " loss = \"binary_crossentropy\",\n",
        " metrics = [\"accuracy\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RckYtYiNHOw8",
        "outputId": "d2757d25-539b-4eae-c0d8-76b74c175d36"
      },
      "outputs": [],
      "source": [
        "# Now we're able to train our model. We'll do this with a batch_size of 500 and only for two epochs because I recognized that the model overfits if we train it longer.\n",
        "# batch size defines the number of samples that will be propagated through the network.\n",
        "# For instance, let's say you have 1050 training samples and you want to set up a batch_size equal to 100. \n",
        "# The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network. \n",
        "# Next, it takes the second 100 samples (from 101st to 200th) and trains the network again. \n",
        "# We can keep doing this procedure until we have propagated all samples through of the network. \n",
        "# Problem might happen with the last set of samples. In our example, we've used 1050 which is not divisible by 100 without remainder.\n",
        "# The simplest solution is just to get the final 50 samples and train the network.\n",
        "##The goal is to find the number of epochs that results in good performance on a validation dataset without overfitting to the training data.\n",
        "results = model.fit(\n",
        " X_train, y_train,\n",
        " epochs= 2,\n",
        " batch_size = 500,\n",
        " validation_data = (X_test, y_test),\n",
        " callbacks=[callback]\n",
        ") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94oaobMeHQ97",
        "outputId": "d5446ce6-d102-4594-dde9-db3f73c37840"
      },
      "outputs": [],
      "source": [
        "# Let's check mean accuracy of our model\n",
        "print(np.mean(results.history[\"val_accuracy\"])) # Good model should have a mean accuracy significantly higher than 50%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "gXT3sRG8HYHo",
        "outputId": "cae57df6-9d19-478d-a4ad-38b6b8384014"
      },
      "outputs": [],
      "source": [
        "#Let's plot training history of our model\n",
        "\n",
        "# list all data in history\n",
        "print(results.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(results.history['accuracy']) #Plots the training accuracy of the model at each epoch.\n",
        "plt.plot(results.history['val_accuracy']) #Plots the validation accuracy of the model at each epoch.\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(results.history['loss'])\n",
        "plt.plot(results.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pdn0QM_DT_Xa",
        "outputId": "c07c10bc-bb22-4c42-e87a-c3af1e49e349"
      },
      "outputs": [],
      "source": [
        "model.predict(X_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
